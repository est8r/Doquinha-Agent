{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"gglobo\"\n",
    "REGION = \"us-east4\"\n",
    "GSC_BUCKET = \"gs://raw_doquinha\"\n",
    "\n",
    "# Initialize the SDK for Vertex AI Agent Engine\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=GSC_BUCKET,\n",
    ")\n",
    "\n",
    "AI_MODEL = \"gemini-2.5-flash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the Tool to be used: RAG Engine\n",
    "\n",
    "from vertexai.preview import rag\n",
    "\n",
    "EMBEDDING_MODEL = \"publishers/google/models/text-embedding-005\"\n",
    "\n",
    "# Define the backend configuration for the RAG Vector Database\n",
    "backend_config = rag.RagVectorDbConfig(\n",
    "    rag_embedding_model_config=rag.RagEmbeddingModelConfig(\n",
    "        vertex_prediction_endpoint=rag.VertexPredictionEndpoint(\n",
    "            publisher_model=EMBEDDING_MODEL\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the RAG corpus\n",
    "rag_corpus = rag.create_corpus(\n",
    "    display_name=\"doquinha-rag-corpus\", backend_config=backend_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating list of doc files in GCS bucket\n",
    "from google.cloud import storage\n",
    "\n",
    "my_bucket = GSC_BUCKET[5:]  # Extract bucket name from GSC path\n",
    "\n",
    "def list_files_by_extension(bucket_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    files_by_extension = {}\n",
    "\n",
    "    for blob in blobs:\n",
    "        name = blob.name\n",
    "        ext = '.' + name.split('.')[-1] if '.' in name else ''\n",
    "        full_name = 'gs://' + bucket_name + '/' + name\n",
    "        files_by_extension.setdefault(ext, []).append(full_name)\n",
    "\n",
    "    return files_by_extension\n",
    "\n",
    "docs = list_files_by_extension(my_bucket)\n",
    "\n",
    "paths_md = docs[\".md\"]\n",
    "paths_png = docs[\".png\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Markdown files into the RAG corpus\n",
    "rag.import_files(\n",
    "    corpus_name=rag_corpus.name,\n",
    "    paths=paths_md,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Define the LLM parser configuration for parsing PNG files\n",
    "llm_parser_config = rag.LlmParserConfig(\n",
    "    model_name = AI_MODEL,\n",
    "    # max_parsing_requests_per_min=MAX_PARSING_REQUESTS_PER_MIN, # Optional\n",
    "    # custom_parsing_prompt=CUSTOM_PARSING_PROMPT, # Optional\n",
    ")\n",
    "\n",
    "# Importing PNG files into the RAG corpus\n",
    "rag.import_files(\n",
    "    corpus_name=rag_corpus.name,\n",
    "    paths=paths_png,\n",
    "    llm_parser=llm_parser_config,\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=200,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_corpus.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview import rag\n",
    "rag_corpus = rag.get_corpus(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.tools.retrieval.vertex_ai_rag_retrieval import VertexAiRagRetrieval\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the instructions for the root agent\n",
    "def return_instructions_root() -> str:\n",
    "    instruction_prompt_v1 = \"\"\"\n",
    "        You are an AI assistant with access to specialized corpus of documents, containing markdown and image files.\n",
    "        Your role is to provide accurate and concise answers to questions based\n",
    "        on documents that are retrievable using ask_vertex_retrieval. If you believe\n",
    "        the user is just chatting and having casual conversation, don't use the retrieval tool.\n",
    "\n",
    "        But if the user is asking a specific question about a knowledge they expect you to have,\n",
    "        you can use the retrieval tool to fetch the most relevant information.\n",
    "        \n",
    "        If you are not certain about the user intent, make sure to ask clarifying questions\n",
    "        before answering. Once you have the information you need, you can use the retrieval tool\n",
    "        If you cannot provide an answer, clearly explain why.\n",
    "\n",
    "        Do not answer questions that are not related to the corpus.\n",
    "        When crafting your answer, you may use the retrieval tool to fetch details\n",
    "        from the corpus. Make sure to cite the source of the information.\n",
    "        \n",
    "        Citation Format Instructions:\n",
    " \n",
    "        When you provide an answer, you must also add one or more citations **at the end** of\n",
    "        your answer. If your answer is derived from only one retrieved chunk,\n",
    "        include exactly one citation. If your answer uses multiple chunks\n",
    "        from different files, provide multiple citations. If two or more\n",
    "        chunks came from the same file, cite that file only once.\n",
    "\n",
    "        **How to cite:**\n",
    "        - Use the retrieved chunk's `title` to reconstruct the reference.\n",
    "        - Include the document title and section if available.\n",
    "        - For web resources, include the full URL when available.\n",
    " \n",
    "        Format the citations at the end of your answer under a heading like\n",
    "        \"Citations\" or \"References.\" For example:\n",
    "        \"Citations:\n",
    "        1) RAG Guide: Implementation Best Practices\n",
    "        2) Advanced Retrieval Techniques: Vector Search Methods\"\n",
    "\n",
    "        Do not reveal your internal chain-of-thought or how you used the chunks.\n",
    "        Simply provide concise and factual answers, and then list the\n",
    "        relevant citation(s) at the end. If you are not certain or the\n",
    "        information is not available, clearly state that you do not have\n",
    "        enough information.\n",
    "        \"\"\"\n",
    "    return instruction_prompt_v1\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create the Vertex AI RAG retrieval tool\n",
    "ask_vertex_retrieval = VertexAiRagRetrieval(\n",
    "    name='retrieve_rag_documentation',\n",
    "    description=(\n",
    "        'Use this tool to retrieve documentation and reference materials for the question from the RAG corpus,'\n",
    "    ),\n",
    "    rag_resources=[\n",
    "        rag.RagResource(\n",
    "            rag_corpus=rag_corpus.name\n",
    "        )\n",
    "    ],\n",
    "    similarity_top_k=10,\n",
    "    vector_distance_threshold=0.6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.genai import types\n",
    "\n",
    "# Define the safety settings for the model\n",
    "safety_settings = [\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=types.HarmBlockThreshold.OFF,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Specify content generation parameters\n",
    "generate_content_config = types.GenerateContentConfig(\n",
    "   safety_settings=safety_settings,\n",
    "   temperature=0.28,\n",
    "   max_output_tokens=1000,\n",
    "   top_p=0.95,\n",
    ")\n",
    "\n",
    "# Create the root agent with the defined instructions and tools\n",
    "root_agent = Agent(\n",
    "    model=AI_MODEL,\n",
    "    name='doquinha_root_agent',\n",
    "    generate_content_config=generate_content_config,  # Optional.\n",
    "    instruction=return_instructions_root(),\n",
    "    tools=[\n",
    "        ask_vertex_retrieval,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.reasoning_engines import AdkApp\n",
    "\n",
    "# Create the AdkApp instance with the root agent\n",
    "app = AdkApp(agent=root_agent)\n",
    "\n",
    "# Testing local app\n",
    "USER_ID = \"test_user\"\n",
    "\n",
    "for event_local in app.stream_query(\n",
    "    user_id=USER_ID,\n",
    "    message=\"O que são videoviews?\",\n",
    "):\n",
    "    print(event_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deploying the agent\n",
    "\n",
    "from vertexai import agent_engines\n",
    "\n",
    "# Create the Agent Engine for the root agent using a service account\n",
    "# remote_app = agent_engines.create(\n",
    "#     agent_engine=root_agent,\n",
    "#     display_name=\"doquinha\",\n",
    "#     requirements=[\n",
    "#         \"google-cloud-aiplatform[adk,agent_engines]\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Create the Agent Engine for the root agent using a service account\n",
    "remote_app_sa = agent_engines.create(\n",
    "    agent_engine=root_agent,\n",
    "    display_name=\"doquinha_sa\",\n",
    "    requirements=[\n",
    "        \"google-cloud-aiplatform[adk,agent_engines]\"\n",
    "    ],\n",
    "    service_account=sa_account\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(remote_app.resource_name)\n",
    "print(remote_app_sa.resource_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote_app = agent_engines.get(\n",
    "\n",
    "remote_app_sa = agent_engines.get(\n",
    "    name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the remote app for the agent\n",
    "\n",
    "from vertexai import agent_engines\n",
    "\n",
    "remote_session = remote_app_sa.create_session(user_id=USER_ID)\n",
    "async for event_remote in remote_app_sa.async_stream_query(\n",
    "    user_id=USER_ID,\n",
    "    session_id=remote_session[\"id\"],\n",
    "    message=\"O que são videoviews?\"\n",
    "):\n",
    "    print(event_remote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote_app.delete(force=True)\n",
    "# remote_app_sa.delete(force=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
